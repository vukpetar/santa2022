{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407514a1",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from environment import Santa2022Environment\n",
    "from utils import *\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90466a2",
   "metadata": {},
   "source": [
    "# Load Image of Christmas card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c602a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image = pd.read_csv(\"image.csv\")\n",
    "image = df_to_image(df_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbc71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2da64",
   "metadata": {},
   "source": [
    "# Load submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_confs = []\n",
    "for sub_file in Path(\"./submissions\").glob(\"*.csv\"):\n",
    "    s = pd.read_csv(sub_file.as_posix())\n",
    "    list_of_confs = s.apply(lambda x: [list(map(int, link.split())) for link in x.configuration.split(\";\")], axis=1).tolist()\n",
    "    all_confs.append(list_of_confs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e70b95",
   "metadata": {},
   "source": [
    "# Define env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3381de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "env = Santa2022Environment(image, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b609220",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, values, actions = [], [], []\n",
    "for confs in all_confs:\n",
    "    rewards = []\n",
    "    obs = env.reset()\n",
    "    obs[\"image\"] = obs[\"image\"].transpose([2, 0, 1])\n",
    "    observations.append(obs)\n",
    "    for conf in confs[1:]:\n",
    "        action = env.new_confs.index(conf)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        obs[\"image\"] = obs[\"image\"].transpose([2, 0, 1])\n",
    "        observations.append(obs)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if len(rewards) == max_iter:\n",
    "            values_array = discounted_cumulative_sums(rewards, 0.99)\n",
    "            values.extend(values_array.tolist())\n",
    "            rewards = []\n",
    "            obs = env.reset(conf)\n",
    "    del observations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e404e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c36f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SantaDataset(Dataset):\n",
    "    \"\"\"Santa dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, observations, actions, values):\n",
    "\n",
    "        self.observations = observations\n",
    "        self.actions = actions\n",
    "        self.values = values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        image = observation[\"image\"]\n",
    "        conf = observation[\"conf\"]\n",
    "        action = self.actions[idx]\n",
    "        reward = self.values[idx]\n",
    "\n",
    "        return image, conf, action, reward\n",
    "\n",
    "limit = len(observations) // BATCH_SIZE * BATCH_SIZE\n",
    "santa_dataest = SantaDataset(observations[:limit], actions[:limit], values[:limit]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(santa_dataest, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the feature extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        n_input_channels: int = 3,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64\n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.rand(1, 3, 257, 257).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, feature_dim), nn.ReLU())\n",
    "        \n",
    "        self.conf_linear = nn.Sequential(nn.Linear(16, feature_dim), nn.ReLU())\n",
    "        \n",
    "        \n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim*2, last_layer_dim_pi)\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim*2, last_layer_dim_vf)\n",
    "        )\n",
    "\n",
    "    def forward(self, images: th.Tensor, confs: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "\n",
    "        image_features = self.linear(self.cnn(images))\n",
    "        conf_features = self.conf_linear(confs)\n",
    "        \n",
    "        features = th.cat((image_features, conf_features), 1)\n",
    "\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = CustomNetwork(feature_dim=128, last_layer_dim_pi=3**8, last_layer_dim_vf=1)\n",
    "\n",
    "criterion_a = nn.CrossEntropyLoss()\n",
    "criterion_v = nn.MSELoss()\n",
    "optimizer = optim.AdamW(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\" # or \"cuda\" if available\n",
    "net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_a = 0.0\n",
    "    running_loss_v = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        im, c, a, v = data\n",
    "\n",
    "        im = im.to(DEVICE).float()\n",
    "        c = c.to(DEVICE).float()\n",
    "        a = a.to(DEVICE)\n",
    "        v = v.to(DEVICE).float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output_a, output_v = net(im, c)\n",
    "        loss_a = criterion_a(output_a, a)\n",
    "        loss_v = criterion_v(output_v, v)\n",
    "        loss = loss_a + loss_v\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = th.max(output_a, 1)\n",
    "        total += a.size(0)\n",
    "        correct += (predicted == a).sum().item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_a += loss_a.item()\n",
    "        running_loss_v += loss_v.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}, a_loss: {running_loss_a / 200:.3f}, v_loss: {running_loss_v / 200:.3f}, accuracy: {100 * correct // total}')\n",
    "            running_loss = 0.0\n",
    "            running_loss_a = 0.0\n",
    "            running_loss_v = 0.0\n",
    "            total = 0.0\n",
    "            correct = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887e953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
